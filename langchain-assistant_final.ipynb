{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"47448f83952941d0abf7c86f8e991861":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_92a561922156492da5c71a61c93e986e","IPY_MODEL_1c5fa13f7b26494992865cc858cd41e3","IPY_MODEL_b16d08ef02a64b809e4b7b5147996db9"],"layout":"IPY_MODEL_36786314174d48c99daebfc9dc4a1186"}},"92a561922156492da5c71a61c93e986e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_be25f9ec15304e6c92a2e5a9ac3b2ea9","placeholder":"​","style":"IPY_MODEL_9046005c10bf43758b86b9390dd28b43","value":"Loading checkpoint shards:   0%"}},"1c5fa13f7b26494992865cc858cd41e3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_a068e121882b42b2989a490df46e3a72","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_663035eafb5c483999a4ca0bf3add0ce","value":0}},"b16d08ef02a64b809e4b7b5147996db9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_62ad58dc6db24bb78379d35ab5d5c38b","placeholder":"​","style":"IPY_MODEL_a275bce38d25494bb1ffe58cb368ea69","value":" 0/2 [00:00&lt;?, ?it/s]"}},"36786314174d48c99daebfc9dc4a1186":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be25f9ec15304e6c92a2e5a9ac3b2ea9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9046005c10bf43758b86b9390dd28b43":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a068e121882b42b2989a490df46e3a72":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"663035eafb5c483999a4ca0bf3add0ce":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"62ad58dc6db24bb78379d35ab5d5c38b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a275bce38d25494bb1ffe58cb368ea69":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12413069,"sourceType":"datasetVersion","datasetId":7828630}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q langchain langchain-community langchain-huggingface transformers faiss-cpu","metadata":{"_uuid":"cac76770-3b4b-40b8-97f5-d55a916a9dbe","_cell_guid":"24dcb118-4c75-4feb-877d-99acddded849","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-16T17:32:43.460952Z","iopub.execute_input":"2025-07-16T17:32:43.461144Z","iopub.status.idle":"2025-07-16T17:32:53.513265Z","shell.execute_reply.started":"2025-07-16T17:32:43.461126Z","shell.execute_reply":"2025-07-16T17:32:53.512359Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\npandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ndataproc-spark-connect 0.7.5 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install torch","metadata":{"_uuid":"3f948e6c-b2c5-4b7e-8716-eb4479f18700","_cell_guid":"ac5c3e50-aae8-44f7-8092-e92ff83ae713","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-16T17:32:53.514715Z","iopub.execute_input":"2025-07-16T17:32:53.514965Z","iopub.status.idle":"2025-07-16T17:34:15.611531Z","shell.execute_reply.started":"2025-07-16T17:32:53.514941Z","shell.execute_reply":"2025-07-16T17:34:15.610651Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m95.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\nSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nfrom huggingface_hub import login\nfrom getpass import getpass\n\n# Try to read token from environment variable\nHF_TOKEN = os.getenv(\"HF_token\") or os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n\nif HF_TOKEN:\n    try:\n        login(token=HF_TOKEN)\n        print(\"Successfully logged in to Hugging Face using environment variable.\")\n    except Exception as e:\n        print(f\"Failed to login using environment token: {e}\")\nelse:\n    # Ask user to enter token securely\n    print(\"Please enter your Hugging Face token:\")\n    HF_TOKEN = getpass(\"Hugging Face token: \").strip()\n    if HF_TOKEN:\n        os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = HF_TOKEN\n        try:\n            login(token=HF_TOKEN)\n            print(\"Successfully logged in to Hugging Face.\")\n        except Exception as e:\n            print(f\"Login failed: {e}\")\n    else:\n        print(\"No token provided. Some models may not be accessible.\")","metadata":{"_uuid":"2637f2e6-1deb-4be7-a131-b0f545f23be8","_cell_guid":"f30a3f4e-ded6-4d1b-b270-8bde9776a778","trusted":true,"collapsed":false,"id":"rN7Q71UdRLta","outputId":"502c3b65-5a43-43ac-f821-c8a21f6aa6a1","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-16T17:34:15.612578Z","iopub.execute_input":"2025-07-16T17:34:15.612860Z","iopub.status.idle":"2025-07-16T17:34:22.992252Z","shell.execute_reply.started":"2025-07-16T17:34:15.612820Z","shell.execute_reply":"2025-07-16T17:34:22.991662Z"}},"outputs":[{"name":"stdout","text":"Please enter your Hugging Face token:\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Hugging Face token:  ········\n"},{"name":"stdout","text":"Successfully logged in to Hugging Face.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nfrom langchain.document_loaders import PyPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import FAISS\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.chains import ConversationalRetrievalChain\nfrom langchain.prompts import PromptTemplate\nfrom langchain_huggingface import HuggingFacePipeline\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nimport torch\nimport os\n\nos.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda/lib64:' + os.environ.get('LD_LIBRARY_PATH', '')","metadata":{"_uuid":"2ccb6212-9e70-4349-977d-38a63cd7fa38","_cell_guid":"8b835449-be6b-48f8-8ace-4042cd4afe8a","trusted":true,"collapsed":false,"id":"qA7OSxFFP8as","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-16T17:34:22.993607Z","iopub.execute_input":"2025-07-16T17:34:22.993868Z","iopub.status.idle":"2025-07-16T17:34:58.814726Z","shell.execute_reply.started":"2025-07-16T17:34:22.993849Z","shell.execute_reply":"2025-07-16T17:34:58.813944Z"}},"outputs":[{"name":"stderr","text":"2025-07-16 17:34:41.588107: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752687281.950928      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752687282.058175      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Set your Hugging Face model name here\nMODEL_NAME = \"google/gemma-2-2b-it\"  \nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(DEVICE)","metadata":{"_uuid":"d56a33f6-d6e3-40b6-8f3e-4576d0093346","_cell_guid":"e2230740-dc02-4874-9e66-f45e9f6c047b","trusted":true,"collapsed":false,"id":"iSQgWEFlQERr","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-16T17:34:58.815523Z","iopub.execute_input":"2025-07-16T17:34:58.816252Z","iopub.status.idle":"2025-07-16T17:34:58.820583Z","shell.execute_reply.started":"2025-07-16T17:34:58.816226Z","shell.execute_reply":"2025-07-16T17:34:58.819927Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Fix 2: Update the LLM setup to use proper chat template\ndef setup_llm(model_name=\"google/gemma-2-2b-it\"):\n    \"\"\"Setup the Hugging Face LLM pipeline with proper chat template\"\"\"\n    \n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        device_map=\"auto\",\n        torch_dtype=torch.float16,\n        token=True,\n    )\n    \n    # Create pipeline with proper settings\n    pipe = pipeline(\n        \"text-generation\",\n        model=model,\n        tokenizer=tokenizer,\n        max_new_tokens=512,\n        do_sample=True,\n        temperature=0.7,\n        pad_token_id=tokenizer.eos_token_id,\n        return_full_text=False,  # This is key - only return the generated part\n    )\n    \n    return HuggingFacePipeline(pipeline=pipe)","metadata":{"_uuid":"31388ac1-bf51-4f18-9e30-9b6bcc6cabd8","_cell_guid":"aa5bbc62-6a7f-4305-8538-ff2c75bd68b6","trusted":true,"collapsed":false,"id":"wwLVUHQlQL1d","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-16T17:34:58.821335Z","iopub.execute_input":"2025-07-16T17:34:58.821626Z","iopub.status.idle":"2025-07-16T17:34:58.838013Z","shell.execute_reply.started":"2025-07-16T17:34:58.821599Z","shell.execute_reply":"2025-07-16T17:34:58.837455Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# ============= DOCUMENT PROCESSING =============\ndef load_and_process_pdf(pdf_path):\n    \"\"\"Load PDF and split into chunks\"\"\"\n    # Load PDF\n    loader = PyPDFLoader(pdf_path)\n    documents = loader.load()\n\n    # Split documents into chunks\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=500,\n        chunk_overlap=100,\n        length_function=len,\n        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n    )\n\n    chunks = text_splitter.split_documents(documents)\n    print(f\"Created {len(chunks)} chunks from the PDF\")\n\n    return chunks","metadata":{"_uuid":"8fab39d2-ae95-4427-a380-c91c1cbdcc64","_cell_guid":"426cf5fa-6107-49a5-aefd-ff3cff55bc1f","trusted":true,"collapsed":false,"id":"WutbEnJXQNRt","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-16T17:34:58.838804Z","iopub.execute_input":"2025-07-16T17:34:58.839426Z","iopub.status.idle":"2025-07-16T17:34:58.859528Z","shell.execute_reply.started":"2025-07-16T17:34:58.839407Z","shell.execute_reply":"2025-07-16T17:34:58.858883Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# ============= EMBEDDINGS AND VECTOR STORE =============\ndef create_vector_store(chunks):\n    \"\"\"Create embeddings and vector store\"\"\"\n    # Initialize embeddings\n    embeddings = HuggingFaceEmbeddings(\n        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n        model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'}\n    )\n\n\n    # Create vector store\n    vectorstore = FAISS.from_documents(chunks, embeddings)\n    print(\"Vector store created successfully\")\n\n    return vectorstore","metadata":{"_uuid":"b7e96393-8683-4405-bd37-aa89402e3998","_cell_guid":"57f63f00-ff48-4f3e-b83d-3a983640338a","trusted":true,"collapsed":false,"id":"T7yIXgXjQRFT","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-16T17:34:58.860181Z","iopub.execute_input":"2025-07-16T17:34:58.860420Z","iopub.status.idle":"2025-07-16T17:34:58.875720Z","shell.execute_reply.started":"2025-07-16T17:34:58.860397Z","shell.execute_reply":"2025-07-16T17:34:58.874998Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def create_prompt_template_with_memory():\n    return PromptTemplate(\n        input_variables=[\"context\", \"chat_history\", \"question\"],\n        template=\"\"\"<bos><start_of_turn>user\nYou are an expert research assistant to a university professor. You have access to research documents and should help with academic inquiries in a scholarly and professional manner.\nYour role:\n- Provide accurate, well-researched responses based on the provided context\n- Cite relevant information from the documents when applicable\n- Maintain an academic tone while being helpful and clear\n- If you don't know something, admit it rather than speculating\n- Help with research analysis, literature review, and academic writing\n\nContext from research documents:\n{context}\n\nPrevious conversation:\n{chat_history}\n\nCurrent question: {question}\n<end_of_turn>\n<start_of_turn>model\n\"\"\")","metadata":{"_uuid":"1c6687ae-3819-4e17-900b-fa2973d9c79c","_cell_guid":"bc7298c5-9bca-4a3c-89ba-d624092613f0","trusted":true,"collapsed":false,"id":"cshHnQ3pQT-F","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-16T17:34:58.876431Z","iopub.execute_input":"2025-07-16T17:34:58.876622Z","iopub.status.idle":"2025-07-16T17:34:58.891118Z","shell.execute_reply.started":"2025-07-16T17:34:58.876600Z","shell.execute_reply":"2025-07-16T17:34:58.890429Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"#Fix 2: Simple memory management functions\ndef format_chat_history(conversation_history):\n    \"\"\"Format conversation history for the prompt\"\"\"\n    if not conversation_history:\n        return \"No previous conversation.\"\n    \n    formatted = []\n    for i in range(0, len(conversation_history), 2):\n        if i + 1 < len(conversation_history):\n            formatted.append(f\"Human: {conversation_history[i]}\")\n            formatted.append(f\"Assistant: {conversation_history[i + 1]}\")\n    \n    return \"\\n\".join(formatted)\n\ndef add_to_memory(conversation_history, question, answer, max_exchanges=6):\n    \"\"\"Add question-answer pair to memory and manage size\"\"\"\n    conversation_history.append(question)\n    conversation_history.append(answer)\n    \n    # Keep only last max_exchanges (each exchange = 2 items: question + answer)\n    max_items = max_exchanges * 2\n    if len(conversation_history) > max_items:\n        conversation_history[:] = conversation_history[-max_items:]\n    \n    return conversation_history\n\ndef clear_memory(conversation_history):\n    \"\"\"Clear conversation memory\"\"\"\n    conversation_history.clear()\n    print(\"Memory cleared!\")","metadata":{"_uuid":"554c8159-235b-4bca-ab5e-29f1571c06cb","_cell_guid":"a81e5ab0-c941-4362-9773-5c7204600bdf","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-16T17:34:58.893108Z","iopub.execute_input":"2025-07-16T17:34:58.893359Z","iopub.status.idle":"2025-07-16T17:34:58.910972Z","shell.execute_reply.started":"2025-07-16T17:34:58.893344Z","shell.execute_reply":"2025-07-16T17:34:58.910481Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_core.runnables import RunnableLambda\nfrom langchain_core.runnables import RunnableMap\n\n\n# Fix 3: Create RAG chain with memory support\ndef create_rag_chain_with_memory(pdf_path, model_name=MODEL_NAME):\n    \"\"\"Create RAG chain that supports memory\"\"\"\n    \n    print(\"Setting up RAG with Memory...\")\n    \n    # Setup components\n    llm = setup_llm(model_name)\n    chunks = load_and_process_pdf(pdf_path)\n    vectorstore = create_vector_store(chunks)\n    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n    \n    # Create prompt template\n    prompt_template = create_prompt_template_with_memory()\n    \n    def format_docs(docs):\n        \"\"\"Format documents for context\"\"\"\n        return \"\\n\\n\".join([f\"Document {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(docs)])\n    \n    # Create the chain\n    qa_chain = (\n        RunnableMap({\n            \"question\": lambda x: x[\"question\"],\n            \"chat_history\": lambda x: x[\"chat_history\"],\n            \"context\": lambda x: format_docs(retriever.get_relevant_documents(x[\"question\"]))\n        })\n        | prompt_template  \n        | llm              \n        | StrOutputParser()\n    )\n    \n    print(\"RAG chain with memory ready!\")\n    return qa_chain","metadata":{"_uuid":"0accdba8-4927-4001-b87d-3b02746b88af","_cell_guid":"72600f5c-56d8-49e1-86ab-f7d54773a10a","trusted":true,"collapsed":false,"id":"3qQmyFcFQYt4","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-16T17:34:58.911576Z","iopub.execute_input":"2025-07-16T17:34:58.911832Z","iopub.status.idle":"2025-07-16T17:34:58.926841Z","shell.execute_reply.started":"2025-07-16T17:34:58.911816Z","shell.execute_reply":"2025-07-16T17:34:58.926351Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Fix 4: Main chat function\ndef ask_question(qa_chain, conversation_history, question):\n    \"\"\"Ask a question with memory support\"\"\"\n    \n    # Format chat history\n    chat_history = format_chat_history(conversation_history)\n    \n    # Generate response\n    response = qa_chain.invoke({\n        \"question\": question,\n        \"chat_history\": chat_history\n    })\n    \n    # Add to memory\n    add_to_memory(conversation_history, question, response)\n    \n    return response\n\n# Fix 5: Utility functions\ndef show_memory(conversation_history):\n    \"\"\"Show current conversation history\"\"\"\n    print(\"=== CONVERSATION HISTORY ===\")\n    formatted = format_chat_history(conversation_history)\n    print(formatted)\n    print(\"==========================\")\n\ndef get_memory_size(conversation_history):\n    \"\"\"Get number of exchanges in memory\"\"\"\n    return len(conversation_history) // 2","metadata":{"_uuid":"a989530f-3f73-4efb-bc2c-b3ae7fdc9120","_cell_guid":"bcc078db-da45-4990-864e-b3adc5083119","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-16T17:34:58.927428Z","iopub.execute_input":"2025-07-16T17:34:58.927602Z","iopub.status.idle":"2025-07-16T17:34:58.946527Z","shell.execute_reply.started":"2025-07-16T17:34:58.927587Z","shell.execute_reply":"2025-07-16T17:34:58.945851Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Usage Example:\nprint(\"=== Testing RAG with Memory - Simple Functions ===\")\n\n# Step 1: Create the RAG chain\nqa_chain = create_rag_chain_with_memory(\n    pdf_path=\"/kaggle/input/sample-v3/attention_is_all_you_need.pdf\",\n    model_name=MODEL_NAME\n)\n\n# Step 2: Initialize conversation history (empty list)\nconversation_history = []\n\n# Step 3: Ask questions\nprint(\"\\n--- starting the conversation ---\")\nquestion0 = \"hello how you doing today?\"\nresponse0 = ask_question(qa_chain, conversation_history, question0)\nprint(f\"Q: {question0}\")\nprint(f\"A: {response0}\")\n\nprint(\"\\n--- First Question ---\")\nquestion1 = \"What is the difference between attention and RNN?\"\nresponse1 = ask_question(qa_chain, conversation_history, question1)\nprint(f\"Q: {question1}\")\nprint(f\"A: {response1}\")\n\nprint(\"\\n--- Second Question (with memory) ---\")\nquestion2 = \"Which is better?\"\nresponse2 = ask_question(qa_chain, conversation_history, question2)\nprint(f\"Q: {question2}\")\nprint(f\"A: {response2}\")\n\nprint(\"\\n--- Third Question ---\")\nquestion3 = \"Can you give me more details about the better one?\"\nresponse3 = ask_question(qa_chain, conversation_history, question3)\nprint(f\"Q: {question3}\")\nprint(f\"A: {response3}\")\n\n# Step 4: Show memory\nshow_memory(conversation_history)","metadata":{"_uuid":"6e70e50b-9d5f-4403-af46-0339e6db9ee6","_cell_guid":"8a1cf225-5431-45b7-b554-744e7dd2ade4","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-16T17:34:58.947283Z","iopub.execute_input":"2025-07-16T17:34:58.947536Z","iopub.status.idle":"2025-07-16T17:40:36.662247Z","shell.execute_reply.started":"2025-07-16T17:34:58.947515Z","shell.execute_reply":"2025-07-16T17:40:36.661146Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"=== Testing RAG with Memory - Simple Functions ===\nSetting up RAG with Memory...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00388a4f9f9e47af87b8f8b6d02c181f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87fc1e1358b3448789f46e329ab5b528"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b92fb57c43ae479f9280de382ff13a02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6af94e6524c54e0190cba2a5d20f49fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/838 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9362a470df2402b8696a501ceb5c2ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7b66e65e3964b91b385dd59ee407843"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb2f1365ebbd45febba91c9fa3e449b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e695cd9b2574c54aedb264f1a8a5c26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/241M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad11d94d7de84a578cf5bf8680e157b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9072734a446455ba8436799dbc44237"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c55c5d6cdc245878a40a301996851cc"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"Created 103 chunks from the PDF\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/2714205897.py:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n  embeddings = HuggingFaceEmbeddings(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d38ebfc5fd3045789e768da86a95d92d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2abf96118e594594acdb37cda5e6b754"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc0e26ca5f0d4adea3e849df918014ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8b57cb925e442a1b7a2e549d104bc66"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60d87a382bb947b6a3e4d1b77fa9e2ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48b225d37eaf4e5bb48958f701b520eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4083101699f144a4945e4e011b1fe037"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cca70d13dd3c457fa58979db675222c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fbc03dd8d454744a3b5324866cc9651"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"416670e9e9e04597b18bc7128f7904b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43d12b9f15f6422db5ef82721b4e42a1"}},"metadata":{}},{"name":"stdout","text":"Vector store created successfully\nRAG chain with memory ready!\n\n--- starting the conversation ---\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/1152636056.py:31: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n  \"context\": lambda x: format_docs(retriever.get_relevant_documents(x[\"question\"]))\nW0716 17:36:46.311000 36 torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode\nW0716 17:36:46.318000 36 torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode\nskipping cudagraphs due to skipping cudagraphs due to multiple devices: device(type='cuda', index=0), device(type='cuda', index=1)\n","output_type":"stream"},{"name":"stdout","text":"Q: hello how you doing today?\nA: I am doing well, thank you. How can I assist you today? \n\n\n--- First Question ---\n","output_type":"stream"},{"name":"stderr","text":"skipping cudagraphs due to skipping cudagraphs due to multiple devices: device(type='cuda', index=0), device(type='cuda', index=1)\n","output_type":"stream"},{"name":"stdout","text":"Q: Which is better?\nA: It's not as simple as saying one is \"better\" – both RNNs and Transformer models have strengths and weaknesses, making them suitable for different tasks. \n\nHere's a breakdown of the key differences you asked about:\n\n**RNNs:**\n\n* **Strengths:** RNNs excel at handling sequential data, where the order of input is crucial. They are great for tasks like language modeling, text generation, and speech recognition where understanding context over time is essential. \n* **Weaknesses:** RNNs can struggle with long sequences due to their sequential nature. The \"memory cell\" can lose information after multiple steps, making them less effective at capturing long-term dependencies.\n\n**Transformers (specifically the Transformer architecture used in the mentioned documents):**\n\n* **Strengths:**  Transformers are specifically designed to tackle long-range dependencies in text. Self-attention allows the model to consider all the words in a sentence simultaneously, capturing relationships even between words far apart. This makes them ideal for tasks like machine translation, where understanding the context of entire sentences is critical.\n* **Weaknesses:** Transformers primarily excel at processing unordered data, making them less effective for tasks where the order of input is critical. They can also be computationally expensive to train, requiring more resources than RNNs.\n\n\n**Which is better?**\n\n* **For tasks requiring sequential processing and understanding context over time, RNNs are still a strong option.** For example, in sentiment analysis of a conversation, where the order of words and their sequential relationship are crucial, RNNs may be more appropriate. \n* **For tasks that require capturing long-range dependencies, especially in contexts where the order of words doesn't matter as much (like machine translation), Transformers are often the preferred choice.** The ability to process all words in a sentence simultaneously makes them highly effective at achieving high performance.\n\nUltimately, the \"best\" model depends on the specific task and the constraints of the problem being solved. \n\n\n--- Third Question ---\n","output_type":"stream"},{"name":"stderr","text":"skipping cudagraphs due to skipping cudagraphs due to multiple devices: device(type='cuda', index=0), device(type='cuda', index=1)\n","output_type":"stream"},{"name":"stdout","text":"Q: Can you give me more details about the better one?\nA: You're asking about a very interesting question! It's not just about which is \"better\", but rather the best model for a specific task. \n\nHere's a breakdown of why Transformers are generally considered superior for tasks involving long-range dependencies, and why RNNs have a different place in the field:\n\n**Transformers: The Power of Attention**\n\nThe key to Transformer's success is its **self-attention mechanism**.  Imagine you're reading a sentence. You don't just focus on the words one by one, but you also consider how each word relates to others.  Transformers capture this complex relationship in a single step. \n\nThink about these key points:\n\n* **Parallel Processing:** Transformers can process all words in a sentence simultaneously, unlike RNNs that process sequentially. This allows them to capture long-range dependencies that RNNs might miss. \n* **Contextual Understanding:** Self-attention enables the model to understand the context of each word within the entire sentence. This is essential for tasks like machine translation, where the meaning of a word depends on the surrounding words. \n* **Efficiency:** The self-attention mechanism is computationally efficient, making the training of Transformers faster and less resource-intensive than RNNs.\n\n**RNNs: The Power of Sequential Processing**\n\nRNNs are still valuable because they excel in tasks where the order of input is crucial:\n\n* **Sequential Information:** RNNs are designed to learn from sequential data, making them suitable for tasks like speech recognition, where the order of sounds is critical for understanding.\n* **Contextual Memory:** The \"memory cell\" in RNNs allows them to retain information from previous steps, making them good at understanding context over time. \n* **Adaptability:**  RNNs can be easily adapted to different tasks by changing the input and output layers, making them versatile for various fields.\n\n**Real-World Examples:**\n\n* **Machine Translation:** Transformers have revolutionized machine translation. Their ability to capture long-range dependencies has significantly improved translation quality.\n* **Text Summarization:** Transformers are used to summarize long documents by capturing the most important information and relationships between words.\n* **Question Answering:** Transformers excel at answering questions by understanding the context of the entire question and the relevant information in the passage.\n\n**Choosing the Right Model:**\n\nThe choice between RNNs and Transformers depends on the specific task and its requirements. \n\n* **For tasks with long-range dependencies (machine translation,\n=== CONVERSATION HISTORY ===\nHuman: hello how you doing today?\nAssistant: I am doing well, thank you. How can I assist you today? \n\nHuman: What is the difference between attention and RNN?\nAssistant: The presented documents describe the use of self-attention mechanisms in Transformer models, contrasting it with traditional Recurrent Neural Networks (RNNs).  \n\nHere's a breakdown of the key differences:\n\n**Self-Attention**\n\n*  **Mechanism**: In self-attention layers, each position within a sequence can attend to *all* other positions within the same sequence. This allows the model to capture long-range dependencies between words in a text, surpassing the limitations of RNNs.\n* **Architecture**:  Self-attention layers use a matrix multiplication operation to compute the relationship between all pairs of inputs, allowing for efficient information exchange.\n* **Benefits**:  These mechanisms are often used in sequence-to-sequence models, like the Transformer, to model dependencies without needing to process sequences in a sequential order.\n\n**Recurrent Neural Networks (RNN)**\n\n* **Mechanism**: RNNs rely on a memory cell that retains information from previous time steps, making them sequential in nature. This allows the model to learn  contextual information over a sequence. \n* **Architecture**: RNNs typically consist of recurrent connections that loop back through the input, allowing information to flow through the network.\n* **Limitations**:  RNNs have limitations in their ability to model long-range dependencies, as information gets gradually lost with each step. This can lead to a bottleneck in capturing context for long sequences.\n\n**In summary:** \n\nWhile both self-attention and RNNs leverage attention mechanisms, they differ in their architectural approach and ability to capture long-range dependencies. Self-attention allows for parallel processing, while RNNs process information sequentially.  \n\nHowever, the Transformer architecture, as described in the provided documents, completely eschews RNNs and utilizes self-attention to capture long-range dependencies without the sequential limitations of RNNs.\n\n\nLet me know if you have any other questions. \n\nHuman: Which is better?\nAssistant: It's not as simple as saying one is \"better\" – both RNNs and Transformer models have strengths and weaknesses, making them suitable for different tasks. \n\nHere's a breakdown of the key differences you asked about:\n\n**RNNs:**\n\n* **Strengths:** RNNs excel at handling sequential data, where the order of input is crucial. They are great for tasks like language modeling, text generation, and speech recognition where understanding context over time is essential. \n* **Weaknesses:** RNNs can struggle with long sequences due to their sequential nature. The \"memory cell\" can lose information after multiple steps, making them less effective at capturing long-term dependencies.\n\n**Transformers (specifically the Transformer architecture used in the mentioned documents):**\n\n* **Strengths:**  Transformers are specifically designed to tackle long-range dependencies in text. Self-attention allows the model to consider all the words in a sentence simultaneously, capturing relationships even between words far apart. This makes them ideal for tasks like machine translation, where understanding the context of entire sentences is critical.\n* **Weaknesses:** Transformers primarily excel at processing unordered data, making them less effective for tasks where the order of input is critical. They can also be computationally expensive to train, requiring more resources than RNNs.\n\n\n**Which is better?**\n\n* **For tasks requiring sequential processing and understanding context over time, RNNs are still a strong option.** For example, in sentiment analysis of a conversation, where the order of words and their sequential relationship are crucial, RNNs may be more appropriate. \n* **For tasks that require capturing long-range dependencies, especially in contexts where the order of words doesn't matter as much (like machine translation), Transformers are often the preferred choice.** The ability to process all words in a sentence simultaneously makes them highly effective at achieving high performance.\n\nUltimately, the \"best\" model depends on the specific task and the constraints of the problem being solved. \n\nHuman: Can you give me more details about the better one?\nAssistant: You're asking about a very interesting question! It's not just about which is \"better\", but rather the best model for a specific task. \n\nHere's a breakdown of why Transformers are generally considered superior for tasks involving long-range dependencies, and why RNNs have a different place in the field:\n\n**Transformers: The Power of Attention**\n\nThe key to Transformer's success is its **self-attention mechanism**.  Imagine you're reading a sentence. You don't just focus on the words one by one, but you also consider how each word relates to others.  Transformers capture this complex relationship in a single step. \n\nThink about these key points:\n\n* **Parallel Processing:** Transformers can process all words in a sentence simultaneously, unlike RNNs that process sequentially. This allows them to capture long-range dependencies that RNNs might miss. \n* **Contextual Understanding:** Self-attention enables the model to understand the context of each word within the entire sentence. This is essential for tasks like machine translation, where the meaning of a word depends on the surrounding words. \n* **Efficiency:** The self-attention mechanism is computationally efficient, making the training of Transformers faster and less resource-intensive than RNNs.\n\n**RNNs: The Power of Sequential Processing**\n\nRNNs are still valuable because they excel in tasks where the order of input is crucial:\n\n* **Sequential Information:** RNNs are designed to learn from sequential data, making them suitable for tasks like speech recognition, where the order of sounds is critical for understanding.\n* **Contextual Memory:** The \"memory cell\" in RNNs allows them to retain information from previous steps, making them good at understanding context over time. \n* **Adaptability:**  RNNs can be easily adapted to different tasks by changing the input and output layers, making them versatile for various fields.\n\n**Real-World Examples:**\n\n* **Machine Translation:** Transformers have revolutionized machine translation. Their ability to capture long-range dependencies has significantly improved translation quality.\n* **Text Summarization:** Transformers are used to summarize long documents by capturing the most important information and relationships between words.\n* **Question Answering:** Transformers excel at answering questions by understanding the context of the entire question and the relevant information in the passage.\n\n**Choosing the Right Model:**\n\nThe choice between RNNs and Transformers depends on the specific task and its requirements. \n\n* **For tasks with long-range dependencies (machine translation,\n==========================\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"","metadata":{"_uuid":"761c0d2b-c147-4ae3-9e09-4f345a728853","_cell_guid":"271190ad-95af-40d9-83b7-8a27605fccfa","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}